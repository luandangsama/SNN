{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avfoe0PQdcjJ",
        "outputId": "a6f793f5-a244-432f-cc2c-33b9861171ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-18 19:05:03.115037: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2023-12-18 19:05:03.115075: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/luan/works/SNN/env/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import operator\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "import skimage.io as io\n",
        "import random\n",
        "from scipy import ndimage\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Activation, Input, Lambda, Concatenate,Average, Permute, concatenate\n",
        "from tensorflow.keras.datasets import mnist,fashion_mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical, Sequence\n",
        "from scipy import ndimage\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import constraints\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from random import seed\n",
        "import cv2\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Multiply\n",
        "from keras import backend as K\n",
        "import shutil\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiJOrKReeWmh",
        "outputId": "26169dfb-8edb-4d2d-9a52-6604995752c3"
      },
      "outputs": [],
      "source": [
        "from utils.load_data import *\n",
        "from utils.Tea import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8ztaX3d-d_hl"
      },
      "outputs": [],
      "source": [
        "@tf.RegisterGradient(\"CustomRound\")\n",
        "def _const_round_grad(unused_op, grad):\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kJHg3MlzuLjE"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(Sequence):\n",
        "    def __init__(self,\n",
        "                 img_paths,\n",
        "                 labels,\n",
        "                 batch_size=32,\n",
        "                 dim=(224, 224),\n",
        "                 n_channels=3,\n",
        "                 n_classes=4,\n",
        "                 shuffle=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.img_paths = img_paths\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.img_indexes = np.arange(len(self.img_paths))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.img_indexes) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temps = [self.img_indexes[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temps)\n",
        "        return X, y\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.img_paths))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "    def __data_generation(self, list_IDs_temps):\n",
        "        X = []\n",
        "        y = []\n",
        "        for i, ID in enumerate(list_IDs_temps):\n",
        "            X.append(cv2.imread(self.img_paths[ID], 0))\n",
        "            y.append(self.labels[ID])\n",
        "        X = np.array(X, dtype = np.float64)\n",
        "        X = X[:,:,:, np.newaxis]\n",
        "        X /= 255\n",
        "        return X, to_categorical(y, num_classes=self.n_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiBpIlf9x0lF"
      },
      "source": [
        "# CNN-based extract feature + RANC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ScLya1aug876"
      },
      "outputs": [],
      "source": [
        "def build_model_IR(num_cls, input_shape):\n",
        "\n",
        "  inputs = Input(shape=input_shape)\n",
        "\n",
        "  x = Conv2D(filters = 14, kernel_size = 3, strides=(1,1), padding = \"same\")(inputs)\n",
        "  x = Conv2D(filters = 28, kernel_size = 3, strides=(2,2), padding = \"same\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization(axis = 3)(x)\n",
        "  x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "  x = Conv2D(filters = 14, kernel_size = 3, strides=(1,1), padding = \"same\")(x)\n",
        "  x = Conv2D(filters = 28, kernel_size = 3, strides=(2,2), padding = \"same\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization(axis = 3)(x)\n",
        "  x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "  x = Conv2D(filters = 14, kernel_size = 3, strides=(1,1), padding = \"same\")(x)\n",
        "  x = Conv2D(filters = 28, kernel_size = 3, strides=(2,2), padding = \"same\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization(axis = 3)(x)\n",
        "  x = tf.keras.activations.sigmoid(x)\n",
        "  x = tf.keras.layers.MaxPool2D((2,2), strides = 2, padding = 'valid')(x)\n",
        "\n",
        "  out = Flatten()(x)\n",
        "\n",
        "  # out = AdditivePooling(2048)(out)\n",
        "\n",
        "  flattened_inputs_1 = Lambda(lambda x : x[:,      :1*2048])(out)\n",
        "\n",
        "  x1_1  = Lambda(lambda x : x[:,     :256 ])(flattened_inputs_1)\n",
        "  x2_1  = Lambda(lambda x : x[:, 119 : 375 ])(flattened_inputs_1)\n",
        "  x3_1  = Lambda(lambda x : x[:, 238 :494 ])(flattened_inputs_1)\n",
        "  x4_1  = Lambda(lambda x : x[:, 357 : 613])(flattened_inputs_1)\n",
        "  x5_1  = Lambda(lambda x : x[:, 476:732])(flattened_inputs_1)\n",
        "  x6_1  = Lambda(lambda x : x[:, 595:851])(flattened_inputs_1)\n",
        "  x7_1  = Lambda(lambda x : x[:, 714:970])(flattened_inputs_1)\n",
        "  x8_1  = Lambda(lambda x : x[:, 833:1089])(flattened_inputs_1)\n",
        "  x9_1  = Lambda(lambda x : x[:, 952:1208])(flattened_inputs_1)\n",
        "  x10_1  = Lambda(lambda x : x[:, 1071:1327])(flattened_inputs_1)\n",
        "  x11_1  = Lambda(lambda x : x[:, 1190:1446])(flattened_inputs_1)\n",
        "  x12_1  = Lambda(lambda x : x[:, 1309:1565])(flattened_inputs_1)\n",
        "  x13_1  = Lambda(lambda x : x[:, 1428:1684])(flattened_inputs_1)\n",
        "  x14_1  = Lambda(lambda x : x[:, 1547:1803])(flattened_inputs_1)\n",
        "  x15_1  = Lambda(lambda x : x[:, 1666:1922])(flattened_inputs_1)\n",
        "  x16_1  = Lambda(lambda x : x[:, 1704:1960])(flattened_inputs_1)\n",
        "\n",
        "  x1_1  = Tea(64)(x1_1)\n",
        "  x2_1  = Tea(64)(x2_1)\n",
        "  x3_1  = Tea(64)(x3_1)\n",
        "  x4_1  = Tea(64)(x4_1)\n",
        "  x5_1  = Tea(64)(x5_1)\n",
        "  x6_1  = Tea(64)(x6_1)\n",
        "  x7_1  = Tea(64)(x7_1)\n",
        "  x8_1  = Tea(64)(x8_1)\n",
        "  x9_1  = Tea(64)(x9_1)\n",
        "  x10_1  = Tea(64)(x10_1)\n",
        "  x11_1  = Tea(64)(x11_1)\n",
        "  x12_1  = Tea(64)(x12_1)\n",
        "  x13_1  = Tea(64)(x13_1)\n",
        "  x14_1  = Tea(64)(x14_1)\n",
        "  x15_1  = Tea(64)(x15_1)\n",
        "  x16_1  = Tea(64)(x16_1)\n",
        "\n",
        "  x1_1 = concatenate(([x1_1,x2_1,x3_1,x4_1]),axis=1)\n",
        "  x2_1 = concatenate(([x5_1,x6_1,x7_1,x8_1]),axis=1)\n",
        "  x3_1 = concatenate(([x9_1,x10_1,x11_1,x12_1]),axis=1)\n",
        "  x4_1 = concatenate(([x13_1,x14_1,x15_1,x16_1]),axis=1)\n",
        "\n",
        "  x1_1 = Tea(64)(x1_1)\n",
        "  x2_1 = Tea(64)(x2_1)\n",
        "  x3_1 = Tea(64)(x3_1)\n",
        "  x4_1 = Tea(64)(x4_1)\n",
        "\n",
        "  x_out_1 = Concatenate(axis=1)([x1_1,x2_1,x3_1,x4_1])\n",
        "\n",
        "  x_out_1 = Tea(round(256/num_cls)*num_cls)(x_out_1)\n",
        "\n",
        "  x_out = AdditivePooling(num_cls)(x_out_1)\n",
        "\n",
        "  predictions = Activation('softmax')(x_out)\n",
        "\n",
        "  model1 = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "  return model1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zcZwTGCDsd4I"
      },
      "outputs": [],
      "source": [
        "paths = {\n",
        "    9 : \"/home/luan/works/SNN/datasets/IR_9class_5_FOLD_COVER1/\",\n",
        "    23 : \"/home/luan/works/SNN/datasets/SPC_minimize/\"\n",
        "}\n",
        "\n",
        "input_shapes = {\n",
        "    9 : (160, 120, 1),\n",
        "    23 : (120, 160, 1)\n",
        "}\n",
        "\n",
        "path = paths[num_classes]\n",
        "input_shape = input_shapes[num_classes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RvQuO1RPfCs",
        "outputId": "9102e5b3-1607-478a-a925-982c614f4425"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['FOLD_1', 'FOLD_2', 'FOLD_3', 'FOLD_4', 'FOLD_5']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "folds = os.listdir(path)\n",
        "folds = [i for i in folds if \"FOLD\" in i.upper()]\n",
        "folds.sort()\n",
        "folds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdK_qJTrKFMw"
      },
      "source": [
        "### Get images and classes for IR images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jZ5evXeihLY3"
      },
      "outputs": [],
      "source": [
        "x_train = {}\n",
        "y_train = {}\n",
        "\n",
        "x_val = {}\n",
        "y_val = {}\n",
        "\n",
        "for fold in folds:\n",
        "  xtrain = []\n",
        "  ytrain = []\n",
        "\n",
        "  xval = []\n",
        "  yval = []\n",
        "\n",
        "  classes = os.listdir(path + fold + \"/train\")\n",
        "\n",
        "  for cl in classes:\n",
        "    images = os.listdir(path + fold + \"/train/\" + cl)\n",
        "    for img in images:\n",
        "      image = path + fold + \"/train/\" + cl + \"/\" + img\n",
        "      xtrain.append(image)\n",
        "      ytrain.append(int(cl) - 1)\n",
        "\n",
        "  classes = os.listdir(path + fold + \"/val\")\n",
        "  for cl in classes:\n",
        "    images = os.listdir(path + fold + \"/val/\" + cl)\n",
        "    for img in images:\n",
        "      image = path + fold + \"/val/\" + cl + \"/\" + img\n",
        "      xval.append(image)\n",
        "      yval.append(int(cl) - 1)\n",
        "\n",
        "  random.seed(1)\n",
        "  (x_train[fold], y_train[fold]) =  shuffle(xtrain, ytrain)\n",
        "  \n",
        "  random.seed(2)\n",
        "  (x_val[fold], y_val[fold]) = shuffle(xval, yval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZUMjcA4hEbQS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3440/3440 [00:01<00:00, 2793.43it/s]\n",
            "100%|██████████| 3440/3440 [00:01<00:00, 2807.63it/s]\n",
            "100%|██████████| 3440/3440 [00:01<00:00, 2893.46it/s]\n",
            "100%|██████████| 3440/3440 [00:01<00:00, 2908.68it/s]\n",
            "100%|██████████| 3440/3440 [00:01<00:00, 2848.79it/s]\n",
            "100%|██████████| 860/860 [00:00<00:00, 2825.54it/s]\n",
            "100%|██████████| 860/860 [00:00<00:00, 2892.58it/s]\n",
            "100%|██████████| 860/860 [00:00<00:00, 2954.99it/s]\n",
            "100%|██████████| 860/860 [00:00<00:00, 2967.87it/s]\n",
            "100%|██████████| 860/860 [00:00<00:00, 2990.73it/s]\n"
          ]
        }
      ],
      "source": [
        "all_train_data = {}\n",
        "all_train_label = {}\n",
        "\n",
        "all_val_data = {}\n",
        "all_val_label = {}\n",
        "for fold in folds:\n",
        "  train_data = x_train[fold]\n",
        "  train_label = y_train[fold]\n",
        "  data = []\n",
        "  label = []\n",
        "  for i in tqdm(range(len(train_data))):\n",
        "    image = cv2.imread(train_data[i], 0)\n",
        "    data.append(image)\n",
        "    label.append(train_label[i])\n",
        "\n",
        "  data = np.array(data, dtype = np.float64)\n",
        "  data = data[:,:,:, np.newaxis]\n",
        "  label = to_categorical(label, num_classes)\n",
        "\n",
        "  all_train_data[fold] = data\n",
        "  all_train_label[fold] = label\n",
        "\n",
        "for fold in folds:\n",
        "  val_data = x_val[fold]\n",
        "  val_label = y_val[fold]\n",
        "  data = []\n",
        "  label = []\n",
        "  for i in tqdm(range(len(val_data))):\n",
        "    image = cv2.imread(val_data[i], 0)\n",
        "    data.append(image)\n",
        "    label.append(val_label[i])\n",
        "\n",
        "  data = np.array(data, dtype = np.float64)\n",
        "  data = data[:,:,:, np.newaxis]\n",
        "  label = to_categorical(label, num_classes)\n",
        "\n",
        "  all_val_data[fold] = data\n",
        "  all_val_label[fold] = label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWmTTJK6KYwI"
      },
      "source": [
        "### Loop train all folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdXmSkSqe76c"
      },
      "outputs": [],
      "source": [
        "for f in range(5):\n",
        "  fold = folds[f]\n",
        "\n",
        "  X_val = all_val_data[fold]\n",
        "  Y_val = all_val_label[fold]\n",
        "\n",
        "  fold_train = folds.copy()\n",
        "  fold_train.remove(fold)\n",
        "\n",
        "  X_train = all_train_data[fold_train[0]]\n",
        "  Y_train = all_train_label[fold_train[0]]\n",
        "\n",
        "  for i in range(1,4):\n",
        "    X_train = np.concatenate((X_train, all_train_data[fold_train[i]]), axis = 0)\n",
        "    Y_train = np.concatenate((Y_train, all_train_label[fold_train[i]]), axis = 0)\n",
        "\n",
        "  print(\"======================================\", fold, \"===================================\")\n",
        "  print(\"X_train shape: \", X_train.shape)\n",
        "\n",
        "  checkpoint_filepath = f'trainings/IR_{num_classes}classes/{num_classes}_class-{fold}'\n",
        "\n",
        "  model = build_model_IR(num_cls=num_classes, input_shape=input_shape)\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(learning_rate = 0.0005),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  if not os.path.exists(checkpoint_filepath):\n",
        "    model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        monitor = \"val_acc\",\n",
        "        save_weights_only=False,\n",
        "        mode = \"max\",\n",
        "        save_best_only = True)\n",
        "\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size=256,\n",
        "              epochs=50,\n",
        "              verbose=1,\n",
        "              callbacks=[model_checkpoint_callback],\n",
        "              validation_data=(X_val, Y_val))\n",
        "\n",
        "for i in range(30):\n",
        "    print(\"Training for: \", fold)\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    score = model.evaluate(X_val, Y_val, verbose=0)\n",
        "    print(\"Best Accuracy so far: \", score[1]*100)\n",
        "\n",
        "    model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_filepath,\n",
        "      monitor = \"val_acc\",\n",
        "      save_weights_only=False,\n",
        "      mode = \"max\",\n",
        "      initial_value_threshold = score[1],\n",
        "      save_best_only = True)\n",
        "\n",
        "    lr = 10**(-3*(np.random.rand()) - 3)\n",
        "    print(\"Learning rate: \", lr)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Adam(learning_rate = lr),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train, Y_train,\n",
        "            batch_size=512,\n",
        "            epochs=30,\n",
        "            verbose=1,\n",
        "            callbacks=[model_checkpoint_callback],\n",
        "            validation_data = (X_val, Y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzVOwbcMKdoY"
      },
      "source": [
        "### Loop train each fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNvbc4czcdxw"
      },
      "outputs": [],
      "source": [
        "fold = 'FOLD_4'\n",
        "\n",
        "X_val = all_val_data[fold]\n",
        "Y_val = all_val_label[fold]\n",
        "\n",
        "fold_train = folds.copy()\n",
        "fold_train.remove(fold)\n",
        "\n",
        "X_train = all_train_data[fold_train[0]]\n",
        "Y_train = all_train_label[fold_train[0]]\n",
        "\n",
        "for i in range(1,4):\n",
        "  X_train = np.concatenate((X_train, all_train_data[fold_train[i]]), axis = 0)\n",
        "  Y_train = np.concatenate((Y_train, all_train_label[fold_train[i]]), axis = 0)\n",
        "\n",
        "\n",
        "checkpoint_filepath = f'trainings/IR_{num_classes}classes/{num_classes}_class-{fold}'\n",
        "\n",
        "model = build_model_IR(num_cls=num_classes, input_shape=input_shape)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate = 0.0005),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "if os.path.exists(checkpoint_filepath):\n",
        "  model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_filepath,\n",
        "      monitor = \"val_acc\",\n",
        "      save_weights_only=False,\n",
        "      mode = \"max\",\n",
        "      save_best_only = True)\n",
        "\n",
        "  model.fit(X_train, Y_train,\n",
        "            batch_size=256,\n",
        "            epochs=50,\n",
        "            verbose=1,\n",
        "            callbacks=[model_checkpoint_callback],\n",
        "            validation_data=(X_val, Y_val))\n",
        "\n",
        "for i in range(50):\n",
        "  print(\"Training for: \", fold)\n",
        "\n",
        "  model.load_weights(checkpoint_filepath)\n",
        "  score = model.evaluate(X_val, Y_val, verbose=0)\n",
        "  print(\"Best Accuracy so far: \", score[1]*100)\n",
        "\n",
        "  model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor = \"val_acc\",\n",
        "    save_weights_only=False,\n",
        "    mode = \"max\",\n",
        "    initial_value_threshold = score[1],\n",
        "    save_best_only = True)\n",
        "\n",
        "  lr = 10**(-3*(np.random.rand()) - 4)\n",
        "  print(\"Learning rate: \", lr)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(learning_rate = lr),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X_train, Y_train,\n",
        "          batch_size=512,\n",
        "          epochs=30,\n",
        "          verbose=1,\n",
        "          callbacks=[model_checkpoint_callback],\n",
        "          validation_data = (X_val, Y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvoIEoVKKtju"
      },
      "source": [
        "### Training using Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUKW4RMg1KH4"
      },
      "outputs": [],
      "source": [
        "train_generator = DataGenerator(xtrain, ytrain, batch_size = 1024, dim = (160,120), n_classes=9, shuffle=True)\n",
        "val_generator = DataGenerator(xval, yval, batch_size=1024, dim = (160, 120), n_classes= 9, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqWYF63z5DUW"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate = 0.001),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iOte1VL4nKC"
      },
      "outputs": [],
      "source": [
        "model.fit(train_generator,\n",
        "          steps_per_epoch=len(train_generator),\n",
        "          epochs=10,\n",
        "          validation_data=val_generator,\n",
        "          validation_steps=len(val_generator))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJoF-P_iK0Mr"
      },
      "source": [
        "### Evaluate all fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FYglRosU-zT",
        "outputId": "6ef4b38b-614b-43f2-e6f0-0a2859117a93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/luan/works/SNN/env/lib/python3.9/site-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-18 19:05:23.378588: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /home/luan/works/SNN/checkpoints/IR_9classes/9_class-FOLD_1: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n",
            "2023-12-18 19:05:23.607595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2023-12-18 19:05:23.607715: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/luan/works/SNN/env/lib/python3.9/site-packages/cv2/../../lib64:\n",
            "2023-12-18 19:05:23.607757: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/luan/works/SNN/env/lib/python3.9/site-packages/cv2/../../lib64:\n",
            "2023-12-18 19:05:23.607790: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/luan/works/SNN/env/lib/python3.9/site-packages/cv2/../../lib64:\n",
            "2023-12-18 19:05:23.609119: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/luan/works/SNN/env/lib/python3.9/site-packages/cv2/../../lib64:\n",
            "2023-12-18 19:05:23.609159: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/luan/works/SNN/env/lib/python3.9/site-packages/cv2/../../lib64:\n",
            "2023-12-18 19:05:23.609289: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2023-12-18 19:05:23.610307: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/home/luan/works/SNN/env/lib/python3.9/site-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOLD_1\n",
            "Test acc =  99.42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-18 19:05:25.520650: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /home/luan/works/SNN/checkpoints/IR_9classes/9_class-FOLD_2: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOLD_2\n",
            "Test acc =  98.37\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-18 19:05:27.490945: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /home/luan/works/SNN/checkpoints/IR_9classes/9_class-FOLD_3: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOLD_3\n",
            "Test acc =  98.84\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-18 19:05:29.330060: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /home/luan/works/SNN/checkpoints/IR_9classes/9_class-FOLD_4: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOLD_4\n",
            "Test acc =  98.02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-18 19:05:31.257157: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /home/luan/works/SNN/checkpoints/IR_9classes/9_class-FOLD_5: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOLD_5\n",
            "Test acc =  99.30\n"
          ]
        }
      ],
      "source": [
        "acc_fold = []\n",
        "for i in folds:\n",
        "  model = build_model_IR(num_cls=num_classes, input_shape=input_shape)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Adam(learning_rate = 0.00005),\n",
        "                  metrics=['accuracy'])\n",
        "  model.load_weights(f'/home/luan/works/SNN/checkpoints/IR_{num_classes}classes/{num_classes}_class-{i}')\n",
        "\n",
        "  X_val = all_val_data[i].copy()\n",
        "  if num_classes == 23:\n",
        "    X_val /= 255\n",
        "  Y_val = all_val_label[i].copy()\n",
        "\n",
        "  score = model.evaluate(X_val, Y_val, verbose=0)\n",
        "  print(i)\n",
        "  print('Test acc = ' , \"{:.2f}\".format(score[1]*100))\n",
        "  acc_fold.append(score[1]*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average:  98.79069805145264\n"
          ]
        }
      ],
      "source": [
        "print(\"Average: \", sum(acc_fold)/len(acc_fold))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ysWW-k7GVeTQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-18 19:05:33.618580: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /home/luan/works/SNN/checkpoints/IR_9classes/9_class-FOLD_1: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n",
            "2023-12-18 19:05:38.401718: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /home/luan/works/SNN/checkpoints/IR_9classes/9_class-FOLD_2: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n",
            "2023-12-18 19:05:43.483008: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /home/luan/works/SNN/checkpoints/IR_9classes/9_class-FOLD_3: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n",
            "2023-12-18 19:05:48.571551: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /home/luan/works/SNN/checkpoints/IR_9classes/9_class-FOLD_4: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n",
            "2023-12-18 19:05:53.701848: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /home/luan/works/SNN/checkpoints/IR_9classes/9_class-FOLD_5: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n"
          ]
        }
      ],
      "source": [
        "all_val = all_val_data[folds[0]]\n",
        "all_label = all_val_label[folds[0]]\n",
        "\n",
        "for i in range(1,5):\n",
        "  all_val = np.concatenate((all_val, all_val_data[folds[i]]), axis = 0)\n",
        "  all_label= np.concatenate((all_label, all_val_label[folds[i]]), axis = 0)\n",
        "accs = []\n",
        "for i in folds:\n",
        "  model = build_model_IR(num_cls=num_classes, input_shape=input_shape)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Adam(learning_rate = 0.00005),\n",
        "                  metrics=['accuracy'])\n",
        "  model.load_weights(f'/home/luan/works/SNN/checkpoints/IR_{num_classes}classes/{num_classes}_class-{i}')\n",
        "\n",
        "  score = model.evaluate(all_val, all_label, verbose=0)\n",
        "  accs.append(score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IE1ykD9e3Pn0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.9806977, 0.9662791, 0.97976744, 0.96790695, 0.9788372]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pdK_qJTrKFMw",
        "1VAXFlX9KMYN",
        "rWmTTJK6KYwI",
        "NhnUsoDZKixu",
        "wvoIEoVKKtju",
        "GJoF-P_iK0Mr"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
