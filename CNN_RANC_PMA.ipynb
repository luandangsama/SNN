{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avfoe0PQdcjJ",
        "outputId": "be59d51f-3836-41d7-8b85-c64835583d80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-18 18:58:08.600738: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2023-12-18 18:58:08.600770: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/luan/works/SNN/env/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import operator\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "import skimage.io as io\n",
        "import random\n",
        "from scipy import ndimage\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Multiply\n",
        "from keras.models import Sequential\n",
        "from keras import backend as K\n",
        "\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Activation, Input, Lambda, Concatenate,Average, Permute, concatenate\n",
        "from tensorflow.keras.datasets import mnist,fashion_mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from scipy import ndimage\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import constraints\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from random import seed\n",
        "import cv2\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiJOrKReeWmh",
        "outputId": "d77f85b2-9d6c-4e46-9695-cc04aa7b9c69"
      },
      "outputs": [],
      "source": [
        "from utils.load_data import *\n",
        "from utils.Tea import *\n",
        "from data.preprocess_pma import  *\n",
        "from modules.baseline import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiBpIlf9x0lF"
      },
      "source": [
        "# CNN-based extract feature + RANC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ScLya1aug876"
      },
      "outputs": [],
      "source": [
        "def build_model_PMA():\n",
        "\n",
        "  inputs = Input(shape=(64, 32, 1,))\n",
        "\n",
        "  x = Conv2D(filters = 8, kernel_size = 3, strides=(1,1), padding = \"same\")(inputs)\n",
        "  x = Conv2D(filters = 16, kernel_size = 3, strides=(2,2), padding = \"same\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization(axis = 3)(x)\n",
        "  x = tf.keras.activations.sigmoid(x)\n",
        "  x = tf.keras.layers.MaxPool2D((2,2), strides = 2, padding = 'same')(x)\n",
        "\n",
        "  flattened_inputs = Flatten()(x)\n",
        "\n",
        "  flattened_inputs_1 = Lambda(lambda x : x[:,      :1*2048])(flattened_inputs)\n",
        "\n",
        "  x1_1  = Lambda(lambda x : x[:,     :256 ])(flattened_inputs_1)\n",
        "  x2_1  = Lambda(lambda x : x[:, 119 : 375 ])(flattened_inputs_1)\n",
        "  x3_1  = Lambda(lambda x : x[:, 238 :494 ])(flattened_inputs_1)\n",
        "  x4_1  = Lambda(lambda x : x[:, 357 : 613])(flattened_inputs_1)\n",
        "  x5_1  = Lambda(lambda x : x[:, 476:732])(flattened_inputs_1)\n",
        "  x6_1  = Lambda(lambda x : x[:, 595:851])(flattened_inputs_1)\n",
        "  x7_1  = Lambda(lambda x : x[:, 714:970])(flattened_inputs_1)\n",
        "  x8_1  = Lambda(lambda x : x[:, 833:1089])(flattened_inputs_1)\n",
        "  x9_1  = Lambda(lambda x : x[:, 952:1208])(flattened_inputs_1)\n",
        "  x10_1  = Lambda(lambda x : x[:, 1071:1327])(flattened_inputs_1)\n",
        "  x11_1  = Lambda(lambda x : x[:, 1190:1446])(flattened_inputs_1)\n",
        "  x12_1  = Lambda(lambda x : x[:, 1309:1565])(flattened_inputs_1)\n",
        "  x13_1  = Lambda(lambda x : x[:, 1428:1684])(flattened_inputs_1)\n",
        "  x14_1  = Lambda(lambda x : x[:, 1547:1803])(flattened_inputs_1)\n",
        "  x15_1  = Lambda(lambda x : x[:, 1666:1922])(flattened_inputs_1)\n",
        "  x16_1  = Lambda(lambda x : x[:, 1785:2041])(flattened_inputs_1)\n",
        "\n",
        "  x1_1  = Tea(64)(x1_1)\n",
        "  x2_1  = Tea(64)(x2_1)\n",
        "  x3_1  = Tea(64)(x3_1)\n",
        "  x4_1  = Tea(64)(x4_1)\n",
        "  x5_1  = Tea(64)(x5_1)\n",
        "  x6_1  = Tea(64)(x6_1)\n",
        "  x7_1  = Tea(64)(x7_1)\n",
        "  x8_1  = Tea(64)(x8_1)\n",
        "  x9_1  = Tea(64)(x9_1)\n",
        "  x10_1  = Tea(64)(x10_1)\n",
        "  x11_1  = Tea(64)(x11_1)\n",
        "  x12_1  = Tea(64)(x12_1)\n",
        "  x13_1  = Tea(64)(x13_1)\n",
        "  x14_1  = Tea(64)(x14_1)\n",
        "  x15_1  = Tea(64)(x15_1)\n",
        "  x16_1  = Tea(64)(x16_1)\n",
        "\n",
        "  x1_1 = concatenate(([x1_1,x2_1,x3_1,x4_1]),axis=1)\n",
        "  x2_1 = concatenate(([x5_1,x6_1,x7_1,x8_1]),axis=1)\n",
        "  x3_1 = concatenate(([x9_1,x10_1,x11_1,x12_1]),axis=1)\n",
        "  x4_1 = concatenate(([x13_1,x14_1,x15_1,x16_1]),axis=1)\n",
        "\n",
        "  x1_1 = Dropout(0.1)(x1_1)\n",
        "  x2_1 = Dropout(0.1)(x2_1)\n",
        "  x3_1 = Dropout(0.1)(x3_1)\n",
        "  x4_1 = Dropout(0.1)(x4_1)\n",
        "\n",
        "  x1_1 = Tea(64)(x1_1)\n",
        "  x2_1 = Tea(64)(x2_1)\n",
        "  x3_1 = Tea(64)(x3_1)\n",
        "  x4_1 = Tea(64)(x4_1)\n",
        "\n",
        "  x_out_1 = Concatenate(axis=1)([x1_1,x2_1,x3_1,x4_1])\n",
        "\n",
        "  x_out_1 = Dropout(0.1)(x_out_1)\n",
        "\n",
        "  x_out_1 = Tea(255)(x_out_1)\n",
        "\n",
        "  x_out = AdditivePooling(17)(x_out_1)\n",
        "\n",
        "  predictions = Activation('softmax')(x_out)\n",
        "\n",
        "  model1 = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "  return model1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaTygqnUhLZ7"
      },
      "outputs": [],
      "source": [
        "model = build_model_PMA()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb2ji_ANhQJC"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate = 0.00005),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load data\n",
        "datasets = load_datasets(path=\"datasets/experiment-i\", type_load=\"17class\", preproc=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjA_k2kxjPQo"
      },
      "outputs": [],
      "source": [
        "sub = \"S1\"\n",
        "x_train, y_train, x_test, y_test = preprocess(datasets=datasets, num_cls=17, sub=\"S1\", hist_equal=True, apply_color_map=False, normalize=True)\n",
        "\n",
        "checkpoint_filepath = f'/home/luan/works/SNN/checkpoints/PMA_17classes/17_class-{sub}'\n",
        "model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor = \"val_acc\",\n",
        "    save_weights_only=False,\n",
        "    mode = \"max\",\n",
        "    save_best_only = True)\n",
        "    #initial_value_threshold = 0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsOe3uOLhX3D"
      },
      "outputs": [],
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=256,\n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          # callbacks=[model_checkpoint_callback],\n",
        "          validation_data=(x_train, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdS79rKl9LkL"
      },
      "outputs": [],
      "source": [
        "model.load_weights(checkpoint_filepath)\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Sub: ', sub)\n",
        "print('Test loss = ' , score[0])\n",
        "print('Test acc = ' , score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loop training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPNqnBNVk-Pe"
      },
      "outputs": [],
      "source": [
        "for i in range(100):\n",
        "  print(\"Training for sub :\",sub)\n",
        "\n",
        "  model.load_weights(checkpoint_filepath)\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "  print(\"Best acc so far: \", score[1])\n",
        "\n",
        "\n",
        "  model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "      filepath= checkpoint_filepath ,\n",
        "      monitor = 'val_acc',\n",
        "      save_best_only = True,\n",
        "      mode = 'max',\n",
        "      initial_value_threshold = score[1],\n",
        "      save_weights_only=False)\n",
        "\n",
        "  seed()\n",
        "  lr = 10**(-3*(np.random.rand()) - 2)\n",
        "  print(\"Learning rate: \", lr)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(learning_rate = lr),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          callbacks=[model_checkpoint_callback],\n",
        "          validation_data = (x_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHzQITvgt4j3"
      },
      "outputs": [],
      "source": [
        "sub_train = [\"S4\",\"S5\"]\n",
        "subjects = [\"S1\",\"S2\",\"S3\",\"S4\",\"S5\",\"S6\",\"S7\",\"S8\",\"S9\",\"S10\",\"S11\",\"S12\",\"S13\"]\n",
        "\n",
        "for sub in sub_train:\n",
        "  checkpoint_filepath = f'/home/luan/works/SNN/checkpoints/PMA_17classes/17_class-{sub}'\n",
        "\n",
        "  x_train, y_train, x_test, y_test = preprocess(datasets=datasets, num_cls=17, sub=sub, hist_equal=True, apply_color_map=False, normalize=True)\n",
        "\n",
        "  model = build_model_PMA()\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(learning_rate = 0.0004),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_filepath,\n",
        "      monitor = \"val_acc\",\n",
        "      save_weights_only=False,\n",
        "      mode = \"max\",\n",
        "      save_best_only = True)\n",
        "\n",
        "  if not os.path.exists(checkpoint_filepath):\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=32,\n",
        "              epochs=50,\n",
        "              verbose=1,\n",
        "              callbacks=[model_checkpoint_callback],\n",
        "              validation_data=(x_test, y_test))\n",
        "  else:\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(\"Best Accuracy so far: \", score[1]*100)\n",
        "\n",
        "    if score[1] < 0.9:\n",
        "      print(\"RETRAINING..................\")\n",
        "      model = build_model_PMA()\n",
        "      seed()\n",
        "      lr = 10**(-3*(np.random.rand()) - 3)\n",
        "\n",
        "      model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "          filepath=checkpoint_filepath,\n",
        "          monitor = \"val_acc\",\n",
        "          save_weights_only=False,\n",
        "          mode = \"max\",\n",
        "          save_best_only = True\n",
        "          initial_value_threshold = score[1])\n",
        "      \n",
        "      model.compile(loss='categorical_crossentropy',\n",
        "                    optimizer=Adam(learning_rate = lr),\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "      model.fit(x_train, y_train,\n",
        "              batch_size=32,\n",
        "              epochs=30,\n",
        "              verbose=1,\n",
        "              callbacks=[model_checkpoint_callback],\n",
        "              validation_data = (x_test,y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for i in range(100):\n",
        "    print(\"Training for sub :\",sub)\n",
        "\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(\"Best Accuracy so far: \", score[1]*100)\n",
        "\n",
        "    model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "        filepath= checkpoint_filepath ,\n",
        "        monitor = 'val_acc',\n",
        "        save_best_only = True,\n",
        "        mode = 'max',\n",
        "        initial_value_threshold = score[1],\n",
        "        save_weights_only=False)\n",
        "\n",
        "    seed()\n",
        "    lr = 10**(-3*(np.random.rand()) - 4)\n",
        "    print(\"Learning rate: \", lr)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Adam(learning_rate = lr),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(x_train, y_train,\n",
        "            batch_size=32,\n",
        "            epochs=30,\n",
        "            verbose=1,\n",
        "            callbacks=[model_checkpoint_callback],\n",
        "            validation_data = (x_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FYglRosU-zT",
        "outputId": "42ac4f15-6c78-4522-ef42-8e1b6586984a"
      },
      "outputs": [],
      "source": [
        "checkpoints = os.listdir('/home/luan/works/SNN/checkpoints/PMA_17classes')\n",
        "\n",
        "for ckpt in checkpoints:\n",
        "\n",
        "  sub = ckpt.split(\"-\")[1]\n",
        "  x_train, y_train, x_test, y_test = preprocess(datasets=datasets, num_cls=17, sub=sub, hist_equal=True, apply_color_map=False, normalize=True)\n",
        "  model = build_model_PMA()\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(learning_rate = 0.00005),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  model.load_weights(f'/home/luan/works/SNN/checkpoints/PMA_17classes/17_class-{sub}')\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "  print('Sub: ', sub)\n",
        "  print('Test loss = ' , score[0])\n",
        "  print('Test acc = ' , score[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K34Yv36q_54K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
